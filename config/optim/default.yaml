optimizer:
  #  Adam-oriented deep learning
  _target_: torch.optim.Adam
  #  These are all default parameters for the Adam optimizer
  lr: 0.0005
  betas: [ 0.9, 0.999 ]
  eps: 1e-08
  weight_decay: 0

use_lr_scheduler: True
lr_scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  factor: 0.6
  patience: 10
  min_lr: 1e-4

  # _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
  # T_0: 10        
  # T_mult: 2     
  # eta_min: 1e-5 


#   _target_: torch.optim.lr_scheduler.StepLR
#   step_size: 20   # 20 epoch마다
#   gamma: 0.5       # lr → lr * 0.5 ]


#   _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#   T_max: 100        # 총 학습 epoch
#   eta_min: 1e-6    # 최소 학습률